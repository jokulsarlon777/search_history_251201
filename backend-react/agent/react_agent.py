"""
ReAct Agent Implementation using LangGraph
"""
import os
import time
import logging
from typing import Literal
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage
from langgraph.graph import StateGraph, END

from agent.state import AgentState
from tools import elasticsearch_search
from tools.elasticsearch_tool import ElasticsearchConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ToolNode ì§ì ‘ êµ¬í˜„
def call_tools(state: AgentState) -> dict:
    """ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ë…¸ë“œ"""
    start_time = time.time()
    messages = state["messages"]
    last_message = messages[-1]

    tool_calls = last_message.tool_calls
    tool_messages = []

    logger.info(f"ğŸ”§ Tool calls: {len(tool_calls)} tools to execute")

    # ê° ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰
    for tool_call in tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]

        logger.info(f"ğŸ”¨ Executing tool: {tool_name} with args: {tool_args}")
        tool_start = time.time()

        # ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ í¬í•¨í•œ ë©”ì‹œì§€ ìƒì„±
        tool_info = f"\nğŸ”§ **Tool í˜¸ì¶œ ì •ë³´:**\n"
        tool_info += f"- ë„êµ¬: `{tool_name}`\n"
        tool_info += f"- íŒŒë¼ë¯¸í„°:\n"
        for key, value in tool_args.items():
            tool_info += f"  - {key}: `{value}`\n"
        tool_info += "\n---\n\n"

        # ë„êµ¬ ì‹¤í–‰
        try:
            if tool_name == "elasticsearch_search":
                result = elasticsearch_search.invoke(tool_args)
            else:
                result = f"Unknown tool: {tool_name}"
                logger.error(f"âŒ Unknown tool requested: {tool_name}")
        except Exception as e:
            result = f"Error executing tool {tool_name}: {str(e)}"
            logger.error(f"âŒ Tool execution error: {str(e)}")

        tool_duration = time.time() - tool_start
        logger.info(f"âœ… Tool {tool_name} completed in {tool_duration:.2f}s")

        # ë„êµ¬ í˜¸ì¶œ ì •ë³´ + ê²°ê³¼ë¥¼ í•¨ê»˜ í¬í•¨
        full_result = tool_info + str(result)

        # ë„êµ¬ ë©”ì‹œì§€ ìƒì„±
        tool_messages.append(
            ToolMessage(
                content=full_result,
                tool_call_id=tool_call["id"]
            )
        )

    total_duration = time.time() - start_time
    logger.info(f"ğŸ“Š All tools executed in {total_duration:.2f}s")

    return {"messages": tool_messages}


def generate_reasoning_prompt() -> str:
    """Generate REASONING_PROMPT with available indices from configuration"""
    try:
        config = ElasticsearchConfig()

        # ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ë±ìŠ¤ ëª©ë¡ ìƒì„±
        index_descriptions = []
        for index_name, index_config in config.index_configs.items():
            display_name = index_config.get("display_name", index_name)
            description = index_config.get("description", "")
            index_descriptions.append(f'"{index_name}" ({display_name}): {description}')

        indices_info = "\n   - ".join(index_descriptions) if index_descriptions else "ì„¤ì •ëœ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤"

        return f"""ë‹¹ì‹ ì€ Elasticsearchë¥¼ í™œìš©í•˜ì—¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” AIì…ë‹ˆë‹¤.

## ë„êµ¬
- **elasticsearch_search**: í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
   ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ë±ìŠ¤:
   - {indices_info}

## ì‘ë‹µ í˜•ì‹

### ğŸ¤” Thinking
[1-2ë¬¸ì¥ìœ¼ë¡œ ê²€ìƒ‰ ê³„íš ê°„ë‹¨íˆ ì‘ì„±]

ìœ„ ê³„íšëŒ€ë¡œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”."""
    except Exception as e:
        logger.error(f"âŒ Failed to generate reasoning prompt: {e}")
        # í´ë°± í”„ë¡¬í”„íŠ¸
        return """ë‹¹ì‹ ì€ Elasticsearchë¥¼ í™œìš©í•˜ì—¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” AIì…ë‹ˆë‹¤.

## ë„êµ¬
- **elasticsearch_search**: í‚¤ì›Œë“œë¡œ ê²€ìƒ‰

## ì‘ë‹µ í˜•ì‹

### ğŸ¤” Thinking
[1-2ë¬¸ì¥ìœ¼ë¡œ ê²€ìƒ‰ ê³„íš ê°„ë‹¨íˆ ì‘ì„±]

ìœ„ ê³„íšëŒ€ë¡œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”."""


# System prompts for different stages
REASONING_PROMPT = generate_reasoning_prompt()

ANSWER_PROMPT = """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì •í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

## **STEP 2: ë‹µë³€ ì‘ì„± (ê²€ìƒ‰ ì™„ë£Œ í›„)**

ë°˜ë“œì‹œ ë‹¤ìŒ Markdown í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:

### ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
- ì´ Nê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤

### ğŸ” ìƒì„¸ ë‚´ìš©
[ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ í‘œì‹œ]
- ì°¨ëŸ‰ ë¬¸ì œ: ì°¨ì¢…, ì‹œìŠ¤í…œ, ë¬¸ì œì , ì›ì¸, ëŒ€ì±…ì„ ëª…í™•íˆ ì •ë¦¬
- ê¸°ìˆ  ë¬¸ì„œ: ì£¼ìš” ë‚´ìš©ì„ bullet pointë¡œ ì •ë¦¬

### ğŸ’¡ ê²°ë¡ 
[ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì¢…í•©ì ì¸ ë‹µë³€]

### ğŸ” ì¶”ê°€ë¡œ ê¶ê¸ˆí•˜ì‹¤ ìˆ˜ ìˆëŠ” ì§ˆë¬¸
[ì´ ë‹µë³€ê³¼ ê´€ë ¨í•˜ì—¬ ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ê¶ê¸ˆí•´í•  ë§Œí•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ 3ê°œë¥¼ ì œì•ˆí•˜ì„¸ìš”]
1. [ê´€ë ¨ ì§ˆë¬¸ 1]
2. [ê´€ë ¨ ì§ˆë¬¸ 2]
3. [ê´€ë ¨ ì§ˆë¬¸ 3]

ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì œê³µí•˜ì„¸ìš”."""


def create_react_agent():
    """
    ReAct ì—ì´ì „íŠ¸ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Returns:
        ì»´íŒŒì¼ëœ LangGraph ê·¸ë˜í”„
    """
    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model="gpt-4o-mini",  # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ mini ëª¨ë¸ ì‚¬ìš©
        temperature=0,
        streaming=True,
    )

    # ë„êµ¬ ë°”ì¸ë”©
    tools = [elasticsearch_search]
    llm_with_tools = llm.bind_tools(tools)

    # ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
    def call_model(state: AgentState) -> dict:
        """LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹¤ìŒ ì•¡ì…˜ ê²°ì •"""
        start_time = time.time()
        from langchain_core.messages import AIMessage
        messages = state["messages"]

        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ë„êµ¬ ê²°ê³¼ì¸ì§€ í™•ì¸
        last_message = messages[-1] if messages else None
        is_after_tool = isinstance(last_message, ToolMessage)

        if is_after_tool:
            # STEP 3: ë„êµ¬ ì‹¤í–‰ í›„ - ìµœì¢… ë‹µë³€ ìƒì„±
            logger.info("ğŸ“ Generating final answer based on tool results")
            answer_start = time.time()

            messages_for_answer = [SystemMessage(content=ANSWER_PROMPT)] + [
                m for m in messages if not isinstance(m, SystemMessage)
            ]
            response = llm.invoke(messages_for_answer)

            answer_duration = time.time() - answer_start
            logger.info(f"âœ… Answer generated in {answer_duration:.2f}s")
            logger.info(f"ğŸ“Š Total call_model duration: {time.time() - start_time:.2f}s")

            return {"messages": [response]}
        else:
            # STEP 1 & 2ë¥¼ ë¶„ë¦¬: Thinking ë¨¼ì €, ê·¸ ë‹¤ìŒ ë„êµ¬ í˜¸ì¶œ
            logger.info("ğŸ¤” Starting thinking phase")
            thinking_start = time.time()

            # ë¨¼ì € Thinkingë§Œ ìƒì„± (ë„êµ¬ ì—†ì´)
            messages_for_thinking = [SystemMessage(content=REASONING_PROMPT)] + [
                m for m in messages if not isinstance(m, SystemMessage)
            ]
            thinking_response = llm.invoke(messages_for_thinking)

            thinking_duration = time.time() - thinking_start
            logger.info(f"ğŸ’¡ Thinking completed in {thinking_duration:.2f}s")

            # Thinking ì‘ë‹µì„ ë©”ì‹œì§€ì— ì¶”ê°€
            messages_with_thinking = messages + [thinking_response]

            # ì´ì œ ë„êµ¬ í˜¸ì¶œ ìƒì„±
            logger.info("ğŸ”§ Generating tool calls")
            tool_call_start = time.time()

            tool_prompt = SystemMessage(content="ì´ì „ Thinkingì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”. í…ìŠ¤íŠ¸ ì‘ë‹µ ì—†ì´ ë„êµ¬ë§Œ í˜¸ì¶œí•˜ì„¸ìš”.")
            messages_for_tools = [tool_prompt] + [
                m for m in messages_with_thinking if not isinstance(m, SystemMessage)
            ]
            tool_response = llm_with_tools.invoke(messages_for_tools)

            tool_call_duration = time.time() - tool_call_start
            num_tool_calls = len(tool_response.tool_calls) if hasattr(tool_response, 'tool_calls') else 0
            logger.info(f"ğŸ”¨ Tool calls generated ({num_tool_calls} calls) in {tool_call_duration:.2f}s")
            logger.info(f"ğŸ“Š Total call_model duration: {time.time() - start_time:.2f}s")

            # ë‘ ì‘ë‹µì„ ëª¨ë‘ ë°˜í™˜
            return {"messages": [thinking_response, tool_response]}

    def should_continue(state: AgentState) -> Literal["tools", "end"]:
        """ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•œì§€ íŒë‹¨"""
        last_message = state["messages"][-1]

        # ë„êµ¬ í˜¸ì¶œì´ ìˆìœ¼ë©´ tools ë…¸ë“œë¡œ, ì—†ìœ¼ë©´ ì¢…ë£Œ
        if last_message.tool_calls:
            return "tools"
        return "end"

    # ê·¸ë˜í”„ êµ¬ì„±
    workflow = StateGraph(AgentState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("agent", call_model)
    workflow.add_node("tools", call_tools)

    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "end": END,
        }
    )
    workflow.add_edge("tools", "agent")

    # ê·¸ë˜í”„ ì»´íŒŒì¼ (LangGraph APIê°€ ìë™ìœ¼ë¡œ persistence ì œê³µ)
    return workflow.compile()


# ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
react_agent = create_react_agent()
